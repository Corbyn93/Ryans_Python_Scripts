{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unauthorized-specification",
   "metadata": {},
   "source": [
    "# Code to analyse 4 point scans vs photon count\n",
    "\n",
    "Here this code is used to analyse the results of the dwell time vs number of repeats for the same photon count rate from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comic-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "english-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path): \n",
    "    # load the ODMR data file in to an np array \n",
    "    data_array = np.loadtxt(file_path)\n",
    "    return(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neither-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(file_path):\n",
    "    # load in the params data. \n",
    "    params_data = pd.read_table(file_path, names = ['Parameter name', 'Parameter value']) \n",
    "    return(params_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "light-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_params():\n",
    "    # formats the plot parameters \n",
    "    plot.rcParams.update({'font.size' : 16})\n",
    "    plot.rcParams.update({'figure.figsize' :[12, 8]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "future-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(reference_data, odmr_data, params_values):\n",
    "    '''Here we re-shape the reference and the ODMR datasets into a 3D array, \n",
    "    based on the number of line scans, frequnecy points and repeats per frequnecy \n",
    "    have been used to build the ODMR dataset.'''\n",
    "    \n",
    "    # Create variables with the frequnecy scan parameters. \n",
    "    num_of_line_scans = int(params_values[4])\n",
    "    num_of_freqs =  int(1000*(params_values[1]-params_values[0])/params_values[2]) + 1\n",
    "    points_per_freq = int(params_values[8])\n",
    "    print(num_of_line_scans, num_of_freqs, points_per_freq)\n",
    "    # Reshape the ODMR and reference datasets. \n",
    "\n",
    "    odmr_shape_3D = odmr_data[0:num_of_freqs*points_per_freq].reshape(num_of_line_scans, num_of_freqs, points_per_freq)\n",
    "    reference_data_3D = reference_data[0:num_of_freqs*points_per_freq].reshape(num_of_line_scans, num_of_freqs, points_per_freq)\n",
    "    \n",
    "    return(odmr_shape_3D, reference_data_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becoming-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(reference_data, odmr_data):\n",
    "    '''Here we normalise the ODMR dataset to the reference data.'''\n",
    "\n",
    "    norm_odmr_data = odmr_data/reference_data\n",
    "    return(norm_odmr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "missing-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_data(odmr_data):\n",
    "    # find the mean data \n",
    "    mean_odmr_data_points =  np.mean(odmr_data, axis = 2)\n",
    "    mean_odmr_data_points_and_linescans =  np.mean(mean_odmr_data_points, axis = 0)\n",
    "    return(mean_odmr_data_points, mean_odmr_data_points_and_linescans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "otherwise-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(file_name, scan_type, date, point_repeats, dwell_time, total_counts, \n",
    "             counts_per_point_repeat, shot_noise, norm_intensity, standard_deviation, standard_error):\n",
    "    '''Save the data in a text file.'''\n",
    "    \n",
    "    save_folder = 'H:\\\\My Documents\\\\Strathclyde_Back_up\\\\Thesis and Planning\\\\Thesis_plots\\\\' + date + '\\\\'\n",
    "    # Checking to see if the folder exists in which to save the data, and creating it if it does not\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "        # Checking the output file exists and creating and initialising it if not\n",
    "    \n",
    "    file_name = save_folder + file_name + scan_type + '_statistics_measurement_for_I_'   \n",
    "    \n",
    "    for i in range(4):\n",
    "        output_data_file = file_name + str(i+1) + '.dat'\n",
    "        if os.path.isfile(output_data_file) == False:\n",
    "            with open(output_data_file, \"w\") as wf:\n",
    "                wf.write('File name'  + '\\t' + 'Number of Repeats' + '\\t' + \n",
    "                         'Dwell time (ms)' + '\\t' + 'Total Counts recorded' + '\\t' \n",
    "                         'Mean counts per single point (N)' + '\\t' + '2.2 x sqrt(N)' + '\\t' \n",
    "                         'Mean Normalised Counts (n)' + '\\t' + 'Standard Deviation of n' + \n",
    "                         '\\t' + 'Standard Error of n' + '\\t' +'\\n')\n",
    "        \n",
    "        print(standard_error[i])\n",
    "        with open(output_data_file, \"a\") as wf:\n",
    "            wf.write(file_name  + '\\t' +\n",
    "                 str(point_repeats) + \"\\t\"  + str(dwell_time) + \"\\t\" + \n",
    "                 str(total_counts[i]) + \"\\t\"  +  \n",
    "                 str(counts_per_point_repeat[i]) + \"\\t\" + str(shot_noise[i]) + \"\\t\" + \n",
    "                 str(norm_intensity[i]) + \"\\t\"  + str(standard_deviation[i]) + \"\\t\" + \n",
    "                 str(standard_error[i]) + '\\n')       \n",
    "        \n",
    "    # Writing contrast dip description to the .dat file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-seeker",
   "metadata": {},
   "source": [
    "### Code starts here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "voluntary-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 10000\n",
      "0.0007679500709968988\n",
      "0.00076338733386595\n",
      "0.0007688819549890493\n",
      "0.0007616118060505941\n"
     ]
    }
   ],
   "source": [
    "# Load in the data file into an np array, and freqeuncy scan parameters \n",
    "# into a pandas array.\n",
    "\n",
    "date = '20211126'\n",
    "scan = 'Point_repeats' # Set if it was multiple point repeats or line scans. \n",
    "set_plot_params()\n",
    "\n",
    "# Load in the data file into an np array, and freqeuncy scan parameters \n",
    "# into a pandas array.\n",
    "folder_path = 'I:\\\\Science\\\\Physics-Nanobiophotonics\\\\Group\\\\User Data\\\\ODMR\\\\ODMR and imaging\\\\2021\\\\' + date + '\\\\'\n",
    "\n",
    "four_point_data_set = '_4_point_referenced_odmr_13' # Short hand for the file name. \n",
    "\n",
    "# Set the file names for all 4 data sets recorded from the experiment. \n",
    "four_point_file_name_ODMR1 = date + four_point_data_set + '_ODMR1.dat'\n",
    "four_point_file_name_ODMR2 = date + four_point_data_set + '_ODMR2.dat'\n",
    "four_point_file_name_Ref1 = date + four_point_data_set  + '_Ref1.dat'\n",
    "four_point_file_name_Ref2 = date + four_point_data_set  + '_Ref2.dat'\n",
    "\n",
    "# Extract the ODMR and reference datasets from the data we are looking at. \n",
    "four_point_odmr_data = load_data(folder_path + four_point_file_name_ODMR1)\n",
    "four_point_reference_data = load_data(folder_path + four_point_file_name_Ref1)\n",
    "\n",
    "# Get the parameters for the experiment. \n",
    "four_point_params_file = load_params(folder_path + date  + four_point_data_set + '_Params.txt')\n",
    "four_point_params_values = four_point_params_file['Parameter value']\n",
    "\n",
    "# Need to have the four_freqs array layered as follows [F1, F2, F3, F4] due to riggering problems in the software.  \n",
    "four_freqs = [2.860 ,2.862, 2.875, 2.877]\n",
    "\n",
    "# Reshape the data so that it can be used for the following script. \n",
    "four_point_odmr_data, four_point_reference_data = reshape_data(four_point_reference_data, \n",
    "                                                               four_point_odmr_data, four_point_params_values)\n",
    "\n",
    "# Just a little bit of code to decide if we are finding the mean of linescans or point repeats. \n",
    "store = [four_point_odmr_data.shape[0], four_point_odmr_data.shape[2]]\n",
    "if store[0] > store[1]:\n",
    "    a = 0\n",
    "else: \n",
    "    a = 2\n",
    "\n",
    "# find the mean values for the four point ODMR data and reference data. \n",
    "mean_odmr_data = np.mean(four_point_odmr_data, axis = a)\n",
    "total_counts = np.sum(four_point_odmr_data, axis = a)\n",
    "mean_ref_data = np.mean(four_point_reference_data, axis = a)\n",
    "\n",
    "# Normalise the full ODMR dataset and then find the mean of the normalised data for the four freqs. \n",
    "norm_odmr_data = normalise_data(four_point_reference_data, four_point_odmr_data)\n",
    "mean_norm_odmr_data = np.mean(norm_odmr_data, axis = a)\n",
    "std_mean_norm_odmr = np.std(norm_odmr_data, axis = a)\n",
    "standard_error_norm_odmr = std_mean_norm_odmr/np.sqrt(four_point_odmr_data.shape[a])\n",
    "\n",
    "# Generate the error arrays for the mean of the ODMR and reference data. \n",
    "four_points_ODMR_error = 2.2*np.sqrt(mean_odmr_data) # shot noise limit approximation\n",
    "four_points_reference_error = 2.2*np.sqrt(mean_ref_data) # shot noise limit approximation\n",
    "\n",
    "save_data(four_point_file_name_ODMR1, scan, date, four_point_params_values[8], \n",
    "          four_point_params_values[3]/1000, total_counts[0], \n",
    "          mean_odmr_data[0], four_points_ODMR_error[0], \n",
    "          mean_norm_odmr_data[0], standard_error_norm_odmr[0], \n",
    "          standard_error_norm_odmr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hispanic-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 100.0 [[11853769. 11935891. 11967336. 11981158.]] [[118537.69 119358.91 119673.36 119811.58]] [[757.44466438 760.06389495 761.06442723 761.50380643]] [[0.95445057 0.95578411 0.9672886  0.96235014]] [0.00101121 0.00094244 0.00111046 0.00095517] [0.00101121 0.00094244 0.00111046 0.00095517]\n"
     ]
    }
   ],
   "source": [
    "print(four_point_params_values[8], \n",
    "          four_point_params_values[3]/1000, total_counts, \n",
    "          mean_odmr_data, four_points_ODMR_error, \n",
    "          mean_norm_odmr_data, standard_error_norm_odmr[0], \n",
    "          standard_error_norm_odmr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-brass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
